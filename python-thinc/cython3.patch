diff --git a/thinc/backends/cblas.pxd b/thinc/backends/cblas.pxd
index 73cea1f2d2..c7c7a8568e 100644
--- a/thinc/backends/cblas.pxd
+++ b/thinc/backends/cblas.pxd
@@ -2,15 +2,15 @@ from libcpp.memory cimport shared_ptr
 
 ctypedef void (*sgemm_ptr)(bint transA, bint transB, int M, int N, int K,
                            float alpha, const float* A, int lda, const float *B,
-                           int ldb, float beta, float* C, int ldc) nogil
+                           int ldb, float beta, float* C, int ldc) noexcept nogil
 
 
 ctypedef void (*saxpy_ptr)(int N, float alpha, const float* X, int incX,
-                           float *Y, int incY) nogil
+                           float *Y, int incY) noexcept nogil
 
 
 ctypedef void (*daxpy_ptr)(int N, double alpha, const double* X, int incX,
-                           double *Y, int incY) nogil
+                           double *Y, int incY) noexcept nogil
 
 
 # Forward-declaration of the BlasFuncs struct. This struct must be opaque, so
@@ -29,9 +29,9 @@ cdef class CBlas:
 #
 # See https://github.com/explosion/thinc/pull/700 for more information.
 
-cdef daxpy_ptr daxpy(CBlas cblas) nogil
-cdef saxpy_ptr saxpy(CBlas cblas) nogil
-cdef sgemm_ptr sgemm(CBlas cblas) nogil
-cdef void set_daxpy(CBlas cblas, daxpy_ptr daxpy) nogil
-cdef void set_saxpy(CBlas cblas, saxpy_ptr saxpy) nogil
-cdef void set_sgemm(CBlas cblas, sgemm_ptr sgemm) nogil
+cdef daxpy_ptr daxpy(CBlas cblas) noexcept nogil
+cdef saxpy_ptr saxpy(CBlas cblas) noexcept nogil
+cdef sgemm_ptr sgemm(CBlas cblas) noexcept nogil
+cdef void set_daxpy(CBlas cblas, daxpy_ptr daxpy) noexcept nogil
+cdef void set_saxpy(CBlas cblas, saxpy_ptr saxpy) noexcept nogil
+cdef void set_sgemm(CBlas cblas, sgemm_ptr sgemm) noexcept nogil
diff --git a/thinc/backends/cblas.pyx b/thinc/backends/cblas.pyx
index e35169417f..1f86c8c36a 100644
--- a/thinc/backends/cblas.pyx
+++ b/thinc/backends/cblas.pyx
@@ -22,20 +22,20 @@ cdef class CBlas:
         funcs.sgemm = blis.cy.sgemm
         self.ptr = make_shared[BlasFuncs](funcs)
 
-cdef daxpy_ptr daxpy(CBlas cblas) nogil:
+cdef daxpy_ptr daxpy(CBlas cblas) noexcept nogil:
     return deref(cblas.ptr).daxpy
 
-cdef saxpy_ptr saxpy(CBlas cblas) nogil:
+cdef saxpy_ptr saxpy(CBlas cblas) noexcept nogil:
     return deref(cblas.ptr).saxpy
 
-cdef sgemm_ptr sgemm(CBlas cblas) nogil:
+cdef sgemm_ptr sgemm(CBlas cblas) noexcept nogil:
     return deref(cblas.ptr).sgemm
 
-cdef void set_daxpy(CBlas cblas, daxpy_ptr daxpy) nogil:
+cdef void set_daxpy(CBlas cblas, daxpy_ptr daxpy) noexcept nogil:
     deref(cblas.ptr).daxpy = daxpy
 
-cdef void set_saxpy(CBlas cblas, saxpy_ptr saxpy) nogil:
+cdef void set_saxpy(CBlas cblas, saxpy_ptr saxpy) noexcept nogil:
     deref(cblas.ptr).saxpy = saxpy
 
-cdef void set_sgemm(CBlas cblas, sgemm_ptr sgemm) nogil:
+cdef void set_sgemm(CBlas cblas, sgemm_ptr sgemm) noexcept nogil:
     deref(cblas.ptr).sgemm = sgemm
diff --git a/thinc/backends/linalg.pxd b/thinc/backends/linalg.pxd
index 37fb9ea2b9..48334185aa 100644
--- a/thinc/backends/linalg.pxd
+++ b/thinc/backends/linalg.pxd
@@ -29,7 +29,7 @@ cdef class Matrix:
 
 cdef class Vec:
     @staticmethod    
-    cdef inline int arg_max(const weight_t* scores, const int n_classes) nogil:
+    cdef inline int arg_max(const weight_t* scores, const int n_classes) noexcept nogil:
         if n_classes == 2:
             return 0 if scores[0] > scores[1] else 1
         cdef int i
@@ -42,7 +42,7 @@ cdef class Vec:
         return best
 
     @staticmethod
-    cdef inline weight_t max(const weight_t* x, int32_t nr) nogil:
+    cdef inline weight_t max(const weight_t* x, int32_t nr) noexcept nogil:
         if nr == 0:
             return 0
         cdef int i
@@ -53,7 +53,7 @@ cdef class Vec:
         return mode
 
     @staticmethod
-    cdef inline weight_t sum(const weight_t* vec, int32_t nr) nogil:
+    cdef inline weight_t sum(const weight_t* vec, int32_t nr) noexcept nogil:
         cdef int i
         cdef weight_t total = 0
         for i in range(nr):
@@ -61,7 +61,7 @@ cdef class Vec:
         return total
 
     @staticmethod
-    cdef inline weight_t norm(const weight_t* vec, int32_t nr) nogil:
+    cdef inline weight_t norm(const weight_t* vec, int32_t nr) noexcept nogil:
         cdef weight_t total = 0
         for i in range(nr):
             total += vec[i] ** 2
@@ -69,24 +69,24 @@ cdef class Vec:
 
     @staticmethod
     cdef inline void add(weight_t* output, const weight_t* x,
-            weight_t inc, int32_t nr) nogil:
+            weight_t inc, int32_t nr) noexcept nogil:
         memcpy(output, x, sizeof(output[0]) * nr)
         Vec.add_i(output, inc, nr)
 
     @staticmethod
-    cdef inline void add_i(weight_t* vec, weight_t inc, int32_t nr) nogil:
+    cdef inline void add_i(weight_t* vec, weight_t inc, int32_t nr) noexcept nogil:
         cdef int i
         for i in range(nr):
             vec[i] += inc
 
     @staticmethod
     cdef inline void mul(weight_t* output, const weight_t* vec, weight_t scal,
-            int32_t nr) nogil:
+            int32_t nr) noexcept nogil:
         memcpy(output, vec, sizeof(output[0]) * nr)
         Vec.mul_i(output, scal, nr)
 
     @staticmethod
-    cdef inline void mul_i(weight_t* vec, weight_t scal, int32_t nr) nogil:
+    cdef inline void mul_i(weight_t* vec, weight_t scal, int32_t nr) noexcept nogil:
         cdef int i
         IF USE_BLAS:
             blis.cy.scalv(BLIS_NO_CONJUGATE, nr, scal, vec, 1)
@@ -96,12 +96,12 @@ cdef class Vec:
 
     @staticmethod
     cdef inline void pow(weight_t* output, const weight_t* vec, weight_t scal,
-            int32_t nr) nogil:
+            int32_t nr) noexcept nogil:
         memcpy(output, vec, sizeof(output[0]) * nr)
         Vec.pow_i(output, scal, nr)
 
     @staticmethod
-    cdef inline void pow_i(weight_t* vec, const weight_t scal, int32_t nr) nogil:
+    cdef inline void pow_i(weight_t* vec, const weight_t scal, int32_t nr) noexcept nogil:
         cdef int i
         for i in range(nr):
             vec[i] **= scal
@@ -109,43 +109,43 @@ cdef class Vec:
     @staticmethod
     @cython.cdivision(True)
     cdef inline void div(weight_t* output, const weight_t* vec, weight_t scal,
-            int32_t nr) nogil:
+            int32_t nr) noexcept nogil:
         memcpy(output, vec, sizeof(output[0]) * nr)
         Vec.div_i(output, scal, nr)
 
     @staticmethod
     @cython.cdivision(True)
-    cdef inline void div_i(weight_t* vec, const weight_t scal, int32_t nr) nogil:
+    cdef inline void div_i(weight_t* vec, const weight_t scal, int32_t nr) noexcept nogil:
         cdef int i
         for i in range(nr):
             vec[i] /= scal
 
     @staticmethod
-    cdef inline void exp(weight_t* output, const weight_t* vec, int32_t nr) nogil:
+    cdef inline void exp(weight_t* output, const weight_t* vec, int32_t nr) noexcept nogil:
         memcpy(output, vec, sizeof(output[0]) * nr)
         Vec.exp_i(output, nr)
 
     @staticmethod
-    cdef inline void exp_i(weight_t* vec, int32_t nr) nogil:
+    cdef inline void exp_i(weight_t* vec, int32_t nr) noexcept nogil:
         cdef int i
         for i in range(nr):
             vec[i] = exp(vec[i])
 
     @staticmethod
-    cdef inline void reciprocal_i(weight_t* vec, int32_t nr) nogil:
+    cdef inline void reciprocal_i(weight_t* vec, int32_t nr) noexcept nogil:
         cdef int i
         for i in range(nr):
             vec[i] = 1.0 / vec[i]
 
     @staticmethod
-    cdef inline weight_t mean(const weight_t* X, int32_t nr_dim) nogil:
+    cdef inline weight_t mean(const weight_t* X, int32_t nr_dim) noexcept nogil:
         cdef weight_t mean = 0.
         for x in X[:nr_dim]:
             mean += x
         return mean / nr_dim
 
     @staticmethod
-    cdef inline weight_t variance(const weight_t* X, int32_t nr_dim) nogil:
+    cdef inline weight_t variance(const weight_t* X, int32_t nr_dim) noexcept nogil:
         # See https://www.johndcook.com/blog/standard_deviation/
         cdef double m = X[0]
         cdef double v = 0.
@@ -162,7 +162,7 @@ cdef class VecVec:
                          const weight_t* x, 
                          const weight_t* y,
                          weight_t scale,
-                         int32_t nr) nogil:
+                         int32_t nr) noexcept nogil:
         memcpy(output, x, sizeof(output[0]) * nr)
         VecVec.add_i(output, y, scale, nr)
    
@@ -170,7 +170,7 @@ cdef class VecVec:
     cdef inline void add_i(weight_t* x, 
                            const weight_t* y,
                            weight_t scale,
-                           int32_t nr) nogil:
+                           int32_t nr) noexcept nogil:
         cdef int i
         IF USE_BLAS:
             blis.cy.axpyv(BLIS_NO_CONJUGATE, nr, scale, y, 1, x, 1)
@@ -182,7 +182,7 @@ cdef class VecVec:
     cdef inline void batch_add_i(weight_t* x, 
                            const weight_t* y,
                            weight_t scale,
-                           int32_t nr, int32_t nr_batch) nogil:
+                           int32_t nr, int32_t nr_batch) noexcept nogil:
         # For fixed x, matrix of y
         cdef int i, _
         for _ in range(nr_batch):
@@ -192,34 +192,34 @@ cdef class VecVec:
  
     @staticmethod
     cdef inline void add_pow(weight_t* output,
-            const weight_t* x, const weight_t* y, weight_t power, int32_t nr) nogil:
+            const weight_t* x, const weight_t* y, weight_t power, int32_t nr) noexcept nogil:
         memcpy(output, x, sizeof(output[0]) * nr)
         VecVec.add_pow_i(output, y, power, nr)
 
    
     @staticmethod
     cdef inline void add_pow_i(weight_t* x, 
-            const weight_t* y, weight_t power, int32_t nr) nogil:
+            const weight_t* y, weight_t power, int32_t nr) noexcept nogil:
         cdef int i
         for i in range(nr):
             x[i] += y[i] ** power
  
     @staticmethod
     cdef inline void mul(weight_t* output,
-            const weight_t* x, const weight_t* y, int32_t nr) nogil:
+            const weight_t* x, const weight_t* y, int32_t nr) noexcept nogil:
         memcpy(output, x, sizeof(output[0]) * nr)
         VecVec.mul_i(output, y, nr)
    
     @staticmethod
     cdef inline void mul_i(weight_t* x, 
-            const weight_t* y, int32_t nr) nogil:
+            const weight_t* y, int32_t nr) noexcept nogil:
         cdef int i
         for i in range(nr):
             x[i] *= y[i]
 
     @staticmethod
     cdef inline weight_t dot(
-            const weight_t* x, const weight_t* y, int32_t nr) nogil:
+            const weight_t* x, const weight_t* y, int32_t nr) noexcept nogil:
         cdef int i
         cdef weight_t total = 0
         for i in range(nr):
@@ -228,7 +228,7 @@ cdef class VecVec:
  
     @staticmethod
     cdef inline int arg_max_if_true(
-            const weight_t* scores, const int* is_valid, const int n_classes) nogil:
+            const weight_t* scores, const int* is_valid, const int n_classes) noexcept nogil:
         cdef int i
         cdef int best = -1
         for i in range(n_classes):
@@ -238,7 +238,7 @@ cdef class VecVec:
 
     @staticmethod
     cdef inline int arg_max_if_zero(
-            const weight_t* scores, const weight_t* costs, const int n_classes) nogil:
+            const weight_t* scores, const weight_t* costs, const int n_classes) noexcept nogil:
         cdef int i
         cdef int best = -1
         for i in range(n_classes):
@@ -250,7 +250,7 @@ cdef class VecVec:
 cdef class Mat:
     @staticmethod
     cdef inline void mean_row(weight_t* Ex,
-            const weight_t* mat, int32_t nr_row, int32_t nr_col) nogil:
+            const weight_t* mat, int32_t nr_row, int32_t nr_col) noexcept nogil:
         memset(Ex, 0, sizeof(Ex[0]) * nr_col)
         for i in range(nr_row):
             VecVec.add_i(Ex, &mat[i * nr_col], 1.0, nr_col)
@@ -259,7 +259,7 @@ cdef class Mat:
     @staticmethod
     cdef inline void var_row(weight_t* Vx,
             const weight_t* mat, const weight_t* Ex,
-            int32_t nr_row, int32_t nr_col, weight_t eps) nogil:
+            int32_t nr_row, int32_t nr_col, weight_t eps) noexcept nogil:
         # From https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance
         if nr_row == 0 or nr_col == 0:
             return
diff --git a/thinc/backends/numpy_ops.pyx b/thinc/backends/numpy_ops.pyx
index 78eee6ada2..a09f68f9d1 100644
--- a/thinc/backends/numpy_ops.pyx
+++ b/thinc/backends/numpy_ops.pyx
@@ -36,12 +36,12 @@ ctypedef float weight_t
 
 
 cdef extern from "math.h":
-    float logf(float x) nogil
-    float sqrtf(float x) nogil
-    float expf(float x) nogil
-    float tanhf(float x) nogil
-    float sinf(float x) nogil
-    float cosf(float x) nogil
+    float logf(float x) noexcept nogil
+    float sqrtf(float x) noexcept nogil
+    float expf(float x) noexcept nogil
+    float tanhf(float x) noexcept nogil
+    float sinf(float x) noexcept nogil
+    float cosf(float x) noexcept nogil
 
 
 @registry.ops("NumpyOps")
@@ -557,7 +557,7 @@ def add_gradient_noise(float[::1] gradient, weight_t noise_level,
                        dtype='float32')
 
 
-cdef void cpu_position_encode(float* output, float period, int N, int D) nogil:
+cdef void cpu_position_encode(float* output, float period, int N, int D) noexcept nogil:
     cdef float pos, d
     cdef int j
     cdef float dimensions = D
@@ -577,7 +577,7 @@ cdef void cpu_position_encode(float* output, float period, int N, int D) nogil:
 
 cdef void cpu_scatter_add(float* dest,
         const int* indices, const float* src,
-        int nr_id, int nr_col) nogil:
+        int nr_id, int nr_col) noexcept nogil:
     cdef int i
     for i in range(nr_id):
         id_ = indices[i]
@@ -589,7 +589,7 @@ cdef void cpu_scatter_add(float* dest,
 @cython.cdivision(True)
 cdef void _adam_momentum(weight_t* gradient, weight_t* mom1, weight_t* mom2,
         int nr_weight, weight_t beta1, weight_t beta2, weight_t eps,
-        weight_t learn_rate) nogil:
+        weight_t learn_rate) noexcept nogil:
     # Calculate Adam on CPU, fused.
     # Assumes the learning rate adjustment is calculated by the caller;
     # a_t = learn_rate * sqrt(1-beta2**timestep) / (1-beta1**timestep)
@@ -626,7 +626,7 @@ cdef void _adam_momentum(weight_t* gradient, weight_t* mom1, weight_t* mom2,
 
 @cython.cdivision(True)
 cdef void cpu_update_averages(weight_t* ema,
-        const weight_t* weights, int nr_weight, weight_t t, weight_t max_decay) nogil:
+        const weight_t* weights, int nr_weight, weight_t t, weight_t max_decay) noexcept nogil:
     cdef weight_t decay = (1.0 + t) / (10.0 + t)
     if decay > max_decay:
         decay = max_decay
@@ -1014,19 +1014,19 @@ def _untranspose_unsplit_weights(params):
     return numpy.concatenate((Wx.ravel(), bias, Wh.ravel(), zeros))
 
 
-cdef inline float sigmoid(float X) nogil:
+cdef inline float sigmoid(float X) noexcept nogil:
     return 1./(1. + expf(-X))
 
 
-cdef inline float dsigmoid(float y) nogil:
+cdef inline float dsigmoid(float y) noexcept nogil:
     return y*(1-y)
 
 
-cdef inline float dtanh(float y) nogil:
+cdef inline float dtanh(float y) noexcept nogil:
     return 1-y**2
 
 
-cdef void cpu_lstm_activate_fwd(float* gates, int B, int N) nogil:
+cdef void cpu_lstm_activate_fwd(float* gates, int B, int N) noexcept nogil:
     """Apply sigmoid activation in-place to columns 0, 1, 2 and tanh to column 3.
     The data is assumed to have the gates in the last dimension.
     """
@@ -1073,7 +1073,7 @@ cdef void cpu_lstm_activate_fwd(float* gates, int B, int N) nogil:
 
 
 cdef void cpu_lstm_gates_fwd(float* hiddens, float* cells,
-        const float* gates, const float* prevcells, int B, int N) nogil:
+        const float* gates, const float* prevcells, int B, int N) noexcept nogil:
     cdef float hf, hi, ho, hc, ct2, ct3
     cdef int i, b, g, c, h
     g = 0
@@ -1102,7 +1102,7 @@ cdef void cpu_lstm_gates_bwd(
     const float* Ct3,
     const float* Ct2,
     int N
-) nogil:
+) noexcept nogil:
     cdef int i
     cdef float ct2, ct3, hf, hi, ho, hc, tanh_ct3
     cdef float d_ho, d_tanh_ct3, dct3, d_hi, d_hc, d_hf
@@ -1138,7 +1138,7 @@ cdef void MurmurHash3_x86_128_uint64(
     const uint64_t val,
     const uint32_t seed,
     uint32_t *out
-) nogil:
+) noexcept nogil:
     cdef uint64_t h1, h2
 
     h1 = val
diff --git a/thinc/extra/search.pxd b/thinc/extra/search.pxd
index a27ba05259..de51fa1659 100644
--- a/thinc/extra/search.pxd
+++ b/thinc/extra/search.pxd
@@ -53,7 +53,7 @@ cdef class Beam:
 
     cdef int _fill(self, Queue* q, weight_t** scores, int** is_valid) except -1
 
-    cdef inline void* at(self, int i) nogil:
+    cdef inline void* at(self, int i) noexcept nogil:
         return self._states[i].content
 
     cdef int initialize(self, init_func_t init_func, del_func_t del_func, int n, void* extra_args) except -1
@@ -62,7 +62,7 @@ cdef class Beam:
     cdef int check_done(self, finish_func_t finish_func, void* extra_args) except -1
  
 
-    cdef inline void set_cell(self, int i, int j, weight_t score, int is_valid, weight_t cost) nogil:
+    cdef inline void set_cell(self, int i, int j, weight_t score, int is_valid, weight_t cost) noexcept nogil:
         self.scores[i][j] = score
         self.is_valid[i][j] = is_valid
         self.costs[i][j] = cost
diff --git a/thinc/layers/sparselinear.pyx b/thinc/layers/sparselinear.pyx
index 84c17330fb..63e7dab1dc 100644
--- a/thinc/layers/sparselinear.pyx
+++ b/thinc/layers/sparselinear.pyx
@@ -129,7 +129,7 @@ class _finish_linear_update:
 cdef void set_scoresC(float* scores,
         const uint64_t* keys, const float* values, const int32_t* lengths,
         int batch_size, int nr_out, const float* weights, int nr_weight,
-        bint v1_indexing) nogil:
+        bint v1_indexing) noexcept nogil:
     cdef uint32_t idx1, idx2
     cdef uint32_t hash1, hash2
     for length in lengths[:batch_size]:
@@ -160,7 +160,7 @@ cdef void set_scoresC(float* scores,
 cdef void set_gradientC(float* d_weights,
         const uint64_t* keys, const float* values, const int32_t* lengths,
         int batch_size, int nr_out, const float* d_scores, int nr_weight,
-        bint v1_indexing) nogil:
+        bint v1_indexing) noexcept nogil:
     cdef uint32_t idx1, idx2
     cdef uint32_t hash1, hash2
     for length in lengths[:batch_size]:
@@ -186,7 +186,7 @@ cdef void set_gradientC(float* d_weights,
         values += length
 
 
-cdef uint32_t MurmurHash3_x86_32_uint64(uint64_t key, uint32_t seed) nogil:
+cdef uint32_t MurmurHash3_x86_32_uint64(uint64_t key, uint32_t seed) noexcept nogil:
     cdef uint32_t h1 = seed
     cdef uint32_t c1 = 0xcc9e2d51u
     cdef uint32_t c2 = 0x1b873593u
